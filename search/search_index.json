{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#torchview","title":"torchview","text":"<p>Torchview provides visualization of pytorch models in the form of visual graphs. Visualization includes tensors, modules, torch.functions and info such as input/output shapes.</p> <p>Pytorch version of <code>plot_model of keras</code> (and more)</p> <p>Supports PyTorch versions $\\geq$ 1.7.</p>"},{"location":"#useful-features","title":"Useful features","text":""},{"location":"#installation","title":"Installation","text":"<p>First, you need to install graphviz,</p> <pre><code>pip install graphviz\n</code></pre> <p>For python interface of graphiz to work, you need to have dot layout command working in your system. If it isn't already installed, I suggest you run the following depeding on your OS,</p> <p>Debian-based Linux distro (e.g. Ubuntu):</p> <pre><code>apt-get install graphviz\n</code></pre> <p>Windows:</p> <pre><code>choco install graphviz\n</code></pre> <p>macOS</p> <pre><code>brew install graphviz\n</code></pre> <p>see more details here</p> <p>Then, continue with installing torchview using pip</p> <pre><code>pip install torchview\n</code></pre> <p>or if you want via conda</p> <pre><code>conda install -c conda-forge torchview\n</code></pre> <p>or if you want most up-to-date version, install directly from repo</p> <pre><code>pip install git+https://github.com/mert-kurttutan/torchview.git\n</code></pre>"},{"location":"#how-to-use","title":"How To Use","text":"<pre><code>from torchview import draw_graph\n\nmodel = MLP()\nbatch_size = 2\n# device='meta' -&gt; no memory is consumed for visualization\nmodel_graph = draw_graph(model, input_size=(batch_size, 128), device='meta')\nmodel_graph.visual_graph\n</code></pre>"},{"location":"#notebook-examples","title":"Notebook Examples","text":"<p>For more examples, see colab notebooks below,</p> <p>Introduction Notebook: </p> <p>Computer Vision Models: </p> <p>NLP Models: </p> <p>Note: Output graphviz visuals return images with desired sizes. But sometimes, on VSCode, some shapes are being cropped due to large size and svg rendering on by VSCode. To solve this, I suggest you run the following</p> <pre><code>import graphviz\ngraphviz.set_jupyter_format('png')\n</code></pre> <p>This problem does not occur on other platforms e.g. JupyterLab or Google Colab.</p>"},{"location":"#supported-features","title":"Supported Features","text":"<ul> <li>Almost all the models, RNN, Sequentials, Skip Connection, Hugging Face Models</li> <li>Support for meta tensors -&gt; no memory consumption (for very Large models) (pytorch version $\\geq$ 1.13) .</li> <li>Shows operations between tensors (in addition to module calls)</li> <li>Rolling/Unrolling feature. Recursively used modules can be rolled visually, see below.</li> <li>Diverse set of inputs/output types, e.g. nested data structure (dict, list, etc), huggingface tokenizer outputs</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<pre><code>def draw_graph(\n    model: nn.Module,\n    input_data: INPUT_DATA_TYPE | None = None,\n    input_size: INPUT_SIZE_TYPE | None = None,\n    graph_name: str = 'model',\n    depth: int | float = 3,\n    device: torch.device | str | None = None,\n    dtypes: list[torch.dtype] | None = None,\n    mode: str | None = None,\n    strict: bool = True,\n    expand_nested: bool = False,\n    graph_dir: str | None = None,\n    hide_module_functions: bool = True,\n    hide_inner_tensors: bool = True,\n    roll: bool = False,\n    show_shapes: bool = True,\n    save_graph: bool = False,\n    filename: str | None = None,\n    directory: str = '.',\n    **kwargs: Any,\n) -&gt; ComputationGraph:\n    '''Returns visual representation of the input Pytorch Module with\n    ComputationGraph object. ComputationGraph object contains:\n\n    1) Root nodes (usually tensor node for input tensors) which connect to all\n    the other nodes of computation graph of pytorch module recorded during forward\n    propagation.\n\n    2) graphviz.Digraph object that contains visual representation of computation\n    graph of pytorch module. This graph visual shows modules/ module hierarchy,\n    torch_functions, shapes and tensors recorded during forward prop, for examples\n    see documentation, and colab notebooks.\n\n\n    Args:\n        model (nn.Module):\n            Pytorch model to represent visually.\n\n        input_data (data structure containing torch.Tensor):\n            input for forward method of model. Wrap it in a list for\n            multiple args or in a dict or kwargs\n\n        input_size (Sequence of Sizes):\n            Shape of input data as a List/Tuple/torch.Size\n            (dtypes must match model input, default is FloatTensors).\n            Default: None\n\n        graph_name (str):\n            Name for graphviz.Digraph object. Also default name graphviz file\n            of Graph Visualization\n            Default: 'model'\n\n        depth (int):\n            Upper limit for depth of nodes to be shown in visualization.\n            Depth is measured how far is module/tensor inside the module hierarchy.\n            For instance, main module has depth=0, whereas submodule of main module\n            has depth=1, and so on.\n            Default: 3\n\n        device (str or torch.device):\n            Device to place and input tensors. Defaults to\n            gpu if cuda is seen by pytorch, otherwise to cpu.\n            Default: None\n\n        dtypes (list of torch.dtype):\n            Uses dtypes to set the types of input tensor if\n            input size is given.\n\n        mode (str):\n            Mode of model to use for forward prop. Defaults\n            to Eval mode if not given\n            Default: None\n\n        strict (bool):\n            if true, graphviz visual does not allow multiple edges\n            between nodes. Mutiple edge occurs e.g. when there are tensors\n            from module node to module node and hiding those tensors\n            Default: True\n\n        expand_nested(bool):\n            if true shows nested modules with dashed borders\n\n        graph_dir (str):\n            Sets the direction of visual graph\n            'TB' -&gt; Top to Bottom\n            'LR' -&gt; Left to Right\n            'BT' -&gt; Bottom to Top\n            'RL' -&gt; Right to Left\n            Default: None -&gt; TB\n\n        hide_module_function (bool):\n            Determines whether to hide module torch_functions. Some\n            modules consist only of torch_functions (no submodule),\n            e.g. nn.Conv2d.\n            True =&gt; Dont include module functions in graphviz\n            False =&gt; Include modules function in graphviz\n            Default: True\n\n        hide_inner_tensors (bool):\n            Inner tensor is all the tensors of computation graph\n            but input and output tensors\n            True =&gt; Does not show inner tensors in graphviz\n            False =&gt; Shows inner tensors in graphviz\n            Default: True\n\n        roll (bool):\n            If true, rolls recursive modules.\n            Default: False\n\n        show_shapes (bool):\n            True =&gt; Show shape of tensor, input, and output\n            False =&gt; Dont show\n            Default: True\n\n        save_graph (bool):\n            True =&gt; Saves output file of graphviz graph\n            False =&gt; Does not save\n            Default: False\n\n        filename (str):\n            name of the file to store dot syntax representation and\n            image file of graphviz graph. Defaults to graph_name\n\n        directory (str):\n            directory in which to store graphviz output files.\n            Default: .\n\n    Returns:\n        ComputationGraph object that contains visualization of the input\n        pytorch model in the form of graphviz Digraph object\n    '''\n</code></pre>"},{"location":"#examples","title":"Examples","text":""},{"location":"#rolled-version-of-recursive-networks","title":"Rolled Version of Recursive Networks","text":"<pre><code>from torchview import draw_graph\n\nmodel_graph = draw_graph(\n    SimpleRNN(), input_size=(2, 3),\n    graph_name='RecursiveNet',\n    roll=True\n)\nmodel_graph.visual_graph\n</code></pre>"},{"location":"#showhide-intermediate-hidden-tensors-and-functionals","title":"Show/Hide intermediate (hidden) tensors and Functionals","text":"<pre><code># Show inner tensors and Functionals\nmodel_graph = draw_graph(\n    MLP(), input_size=(2, 128),\n    graph_name='MLP',\n    hide_inner_tensors=False,\n    hide_module_functions=False,\n)\n\nmodel_graph.visual_graph\n</code></pre>"},{"location":"#resnet-skip-connection-support-for-torch-operations-nested-modules","title":"ResNet / Skip Connection / Support for Torch operations / Nested Modules","text":"<pre><code>import torchvision\n\nmodel_graph = draw_graph(resnet18(), input_size=(1,3,32,32), expand_nested=True)\nmodel_graph.visual_graph\n</code></pre>"},{"location":"#todo","title":"TODO","text":"<ul> <li>[ ] Display Module parameter info</li> <li>[ ] Support for Graph Neural Network (GNN)</li> <li>[ ] Support for Undirected edges for GNNs</li> <li>[ ] Support for torch-based functions[^1]</li> </ul> <p>[^1]: Here, torch-based functions refers to any function that uses only torch functions and modules. This is more general than modules.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>The Chinese version is kindly provided by @1985312383 (on GitHub).</p> <p>All issues and pull requests are much appreciated! If you are wondering how to build the project:</p> <ul> <li>torchview is actively developed using the latest version of Python.</li> <li>Changes should be backward compatible to Python 3.7, and will follow Python's End-of-Life guidance for old versions.</li> <li>Run <code>pip install -r requirements-dev.txt</code>. We use the latest versions of all dev packages.</li> <li>To run unit tests, run <code>pytest</code>.</li> <li>To update the expected output files, run <code>pytest --overwrite</code>.</li> <li>To skip output file tests, use <code>pytest --no-output</code></li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>Parts related to input processing and validation are taken/inspired from torchinfo repository!!.</li> <li>Many of the software related parts (e.g. testing) are also taken/inspired from torchinfo repository. So big thanks to @TylerYep!!!</li> <li>The algorithm for constructing visual graphs is thanks to <code>__torch_function__</code> and subclassing <code>torch.Tensor</code>. Big thanks to all those who developed this API!!.</li> </ul>"},{"location":"model_gallery/","title":"Model Gallery","text":""},{"location":"model_gallery/#model-gallery","title":"Model Gallery","text":"<p>This page showcases visualizations of classic deep learning models generated using <code>torchview</code>.</p> <p>All images are in SVG vector format. You can click on any image or zoom in your browser to view all details.</p>"},{"location":"model_gallery/#computer-vision","title":"\ud83d\uddbc\ufe0f Computer Vision","text":""},{"location":"model_gallery/#resnet-50","title":"ResNet-50","text":"<p>Residual Network: A classic CNN architecture that solves the training challenges of deep networks through skip connections. The Bottleneck structure is clearly shown in the visualization.</p>"},{"location":"model_gallery/#inception-v3","title":"Inception V3","text":"<p>Inception Architecture: Known for its multi-scale branches (Inception Modules), this architecture is both complex and efficient.</p>"},{"location":"model_gallery/#vision-transformer-vit","title":"Vision Transformer (ViT)","text":"<p>ViT: Applies the Transformer architecture directly to image patches, completely eliminating convolutional operations.</p>"},{"location":"model_gallery/#fcn-fully-convolutional-network","title":"FCN (Fully Convolutional Network)","text":"<p>Fully Convolutional Network: A classic model for semantic segmentation, built on a ResNet-50 backbone.</p>"},{"location":"model_gallery/#natural-language-processing-nlp","title":"\ud83d\udcdd Natural Language Processing (NLP)","text":""},{"location":"model_gallery/#transformer-encoder-bert-style","title":"Transformer Encoder (BERT-style)","text":"<p>Transformer Encoder: Contains Multi-Head Attention and Feed Forward Network, serving as the foundation for models like BERT.</p>"},{"location":"model_gallery/#seq2seq-with-attention","title":"Seq2Seq with Attention","text":"<p>Sequence-to-Sequence with Attention: A classic machine translation architecture. The visualization shows the Encoder (left) and Decoder (right) with the Attention computation process in between (horizontal layout).</p>"},{"location":"model_gallery/#recommender-systems","title":"\ud83d\uded2 Recommender Systems","text":""},{"location":"model_gallery/#wide-deep","title":"Wide &amp; Deep","text":"<p>Wide &amp; Deep: A classic recommendation architecture that combines linear models (Wide, for memorization) with deep neural networks (Deep, for generalization).</p>"},{"location":"model_gallery/#generative-models","title":"\ud83c\udfa8 Generative Models","text":""},{"location":"model_gallery/#vae-variational-autoencoder","title":"VAE (Variational Autoencoder)","text":"<p>Variational Autoencoder: The visualization shows the complete generative pipeline: Encoder \u2192 Reparameterization (sampling) \u2192 Decoder.</p>"},{"location":"model_gallery/#special-architectures","title":"\u2b50 Special Architectures","text":""},{"location":"model_gallery/#wavenet","title":"WaveNet","text":"<p>WaveNet: A generative model for speech synthesis. The visualization clearly shows the stacked residual blocks based on dilated convolutions (horizontal layout).</p>"},{"location":"model_gallery/#gcn-graph-convolutional-network","title":"GCN (Graph Convolutional Network)","text":"<p>Graph Convolutional Network: A neural network for processing graph-structured data.</p>"},{"location":"model_gallery/#how-to-contribute","title":"How to Contribute","text":"<p>If you'd like to add new models to the gallery, please ensure: 1. The model architecture is representative. 2. Use <code>torchview</code> to generate SVG format for clarity. 3. Submit a PR to our repository.</p>"},{"location":"used_by/","title":"Used By","text":""},{"location":"used_by/#used-by-ecosystem","title":"Used By / Ecosystem","text":"<ul> <li> <p>Torch-RecHub (by Datawhale)     ---     A PyTorch-based recommendation system framework focused on standardized training/evaluation and reproducible experiments.</p> <p>Highlights:</p> <ul> <li>30+ classic &amp; modern RecSys models (ranking / matching / multi-task / generative)</li> <li>Modular design (easy to add models, datasets, metrics)</li> <li>Unified pipelines for data loading, training, evaluation</li> <li>ONNX export utilities for deployment</li> </ul> <p>GitHub</p> </li> <li> <p>Add your project     ---     Open a PR adding a new card with:</p> <ul> <li>Project name</li> <li>One-line description</li> <li>Link to the repository / website</li> </ul> </li> </ul>"},{"location":"developer/","title":"Overview","text":""},{"location":"developer/#developer","title":"Developer","text":"<p>This part of the documentation is about the developer part of the library. For instance, it provides a documentaion on the algorithms used, visual illustration to make the algorithms clearer.</p>"},{"location":"developer/docs/","title":"Documentation (MkDocs)","text":""},{"location":"developer/docs/#documentation","title":"Documentation","text":"<p>This is the documentation for the TorchView project. The documentation is built using MkDocs.</p>"},{"location":"developer/docs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"developer/docs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"developer/rolling_graph/","title":"Rolling Graph","text":""},{"location":"developer/rolling_graph/#how-to-identify-nodes-and-roll-recursive-graphs","title":"How to identify nodes and roll recursive graphs ?","text":""},{"location":"developer/rolling_graph/#todo-add-images-clear-explanation","title":"TODO: Add Images, clear explanation","text":"<p>In this document, we describe the main mechanism used to identify the computations nodes (as being the same when rolling recursive nodes) and used to display accordingly.</p> <p>Firstly, note that the main information used to identify nodes is the <code>node_id</code> property of ComputationNodes. For instance, if the node_id of two nodes (probably recursivly used) are the same, they are shown with the same node on the visual graph.</p> <p>Regarding the issue of rolling mechanism, there are simply two modes:</p> <ul> <li><code>Roll=False</code></li> <li><code>Roll=True</code></li> </ul> <p>If <code>Roll=False</code>, each computation step is displayed on the graph uniquely despite whether they are recursively used or not. It is as simple as that. This is done by setting node_id of each node to python id of ComputationNodes (e.g. node_id=id(tensor_node_1)).</p> <p>If <code>Roll=True</code>, then we identify \"recursively\" used modules as being the same on the visual graph. The mechanism to identify nodes depends on type of nodes.</p> <ol> <li> <p>TensorNode: TensorNodes are not rolled at all, and they all are uniquely identified. This choice is made since each tensor is used uniquely unlike modules in recursive networks</p> </li> <li> <p>FunctionNode: Functions are identified by the id of torch function associated with output of this FunctionNode. If these are the same, then two FunctionNodes are identified. The reason behinc choice is as follows. First of all, we cannot only use id of torch function since, for instance, two torch functions used in different places but are the same torch function would be identified (e.g. two skip connection used in different places would lead torch.add in different places). But, visually this is not appealing at all. Imagine two skip connection (far apart) sharing the same node, very ugly. But, there is the choice of input id. Well, this is not really optimal either. Image, you have one tensor and and that this tensor is applied to two different torch.relu calls. You would want to see 2 distinct branch of function nodes coming out of this input node. But, if you identify with input nodes, then there will be only one branch. This is because identification node (input node) is the same for each torch.relu call. Finally. we are left with the choice of output node id. This perfectly makes sense since each output tensor must originate from a unique function call. Meaning, once you tell me the tensor node, I can uniquely tell you function node that it returned. For instance, this leads to 2 distinct branches in the previous example of <code>torch.relu</code>. There is another thing to consider when we hide the inner tensors. It is that output to identify to nodes by wont be <code>TensorNodes</code>. Above, we argued that it all works well in the case identification by output tensornode. The mechanism still works well for output Module or FunctionNodes. The reason for this is similar to the previous argument (input node id does not work and output node id gives your unique way to identify FunctionNode).</p> </li> <li> <p>ModuleNode: For ModuleNode, there two subcases: Stateless and Stateful Modules. Stateless Module refers to any module that has no <code>torch.nn.parameter.Parameter</code>. Usually, the way these modules are used is that they are created once and used in different places and multiple times. So, they pretty much used as a torch function. Therefore, they are identified the same way as FunctionNode. To convince you even more, imagine that they are identified by output id but rather the id of python object itself (like stateful module, see below), then the same e.g. all the ReLUs would be connected to the one shared node, (quite ugly). Secondly, we look at Stateful Module. These are idenfitied by the id of the python object ModuleNode. This is very reasonable if you want to roll the graph, you want to the object (i.e. Module with the same parameters) to appear the same visually.</p> </li> </ol>"},{"location":"developer/torch_function_notes/","title":"__torch_function__ Notes","text":""},{"location":"developer/torch_function_notes/#torch_function","title":"torch_function","text":"<p>This document explains some important point about <code>__torch_function__</code> and its role in this project.</p> <ul> <li> <p><code>__torch_function__</code> API is first introduced in v1.5.0, here</p> </li> <li> <p>Subclass preservations of some important operators introduced here. This property is very important since <code>torchview</code> package keeps track tensor of only <code>RecorderTensor</code> subclass. </p> </li> <li> <p>Some important fixes introdued here. For instance support for <code>F.embedding</code> is included. Otherwise, <code>F.embedding</code> under <code>__torch_function__</code> of subclass would return <code>NotImplemented</code>, leading to <code>torch.Tensor</code>, which is not desired. To prevent this issue (and support pytorch version &lt; 1.9), we added the below code in <code>recorder_tensor.py</code></p> </li> </ul> <pre><code>        # This is necessary for torch version &lt; 1.10\n        if func in [F.linear, F.embedding]:\n            out = nn.parameter.Parameter.__torch_function__(\n                func, types, args, kwargs).as_subclass(RecorderTensor)\n        else:\n            # use original torch_function; otherwise,\n            # it leads to infinite recursive call of torch_function\n            out = super().__torch_function__(func, types, args, kwargs)\n</code></pre> <p>To be precise about the versions,</p> <pre><code>F.linear returns `NotImplemented` for versions 1.7.1, 1.8, 1.9 \nF.embedding returns `NotImplemented` for versions 1.7.1, 1.8, 1.9 \n</code></pre> <p>This package does not support torch version &lt;= 1.6 since torch.Tensor does not have <code>__torch_function__</code> as class methods in these version.</p> <ul> <li>Some other relevant PRs: PR1, PR2</li> </ul>"},{"location":"developer/torch_function_notes/#meta-tensor-related-links","title":"Meta tensor related links","text":"<ul> <li>https://github.com/pytorch/pytorch/blob/orig/release/1.9/torch/_tensor.py</li> <li>https://github.com/pytorch/pytorch/issues/87990</li> </ul>"},{"location":"tutorial/","title":"Overview","text":""},{"location":"tutorial/#tutorial","title":"Tutorial","text":"<p>This is intro for tutorials</p>"},{"location":"tutorial/example_introduction/","title":"Example Introduction","text":"<pre><code>! pip install -q torchview\n! pip install -q -U graphviz\n</code></pre> <pre><code>from torchview import draw_graph\nfrom torch import nn\nimport torch\nimport graphviz\n\n# when running on VSCode run the below command\n# svg format on vscode does not give desired result\ngraphviz.set_jupyter_format('png')\n</code></pre> <pre><code>{\"type\":\"string\"}\n</code></pre>   The purpose of this notebook is to introduce API and notation of torchview package with common use cases.     We start with simple MLP model   <pre><code>class MLP(nn.Module):\n    \"\"\"Multi Layer Perceptron with inplace option.\n    Make sure inplace=true and false has the same visual graph\"\"\"\n\n    def __init__(self, inplace: bool = True) -&gt; None:\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(inplace),\n            nn.Linear(128, 128),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.layers(x)\n        return x\n</code></pre> <pre><code>model_graph_1 = draw_graph(\n    MLP(), input_size=(2, 128),\n    graph_name='MLP',\n    hide_inner_tensors=False,\n    hide_module_functions=False,\n)\n</code></pre> <pre><code>model_graph_1.visual_graph\n</code></pre>   Any visual graph representation of pytorch models provided by torchview package consists of nodes and directed edges (maybe also undirected ones for future releases). Each node is connected by an edge that indicates information flow in the neural network.  There are 3 types of nodes:  -   Tensor Node -   Function Node -   Module Node     1\\) Tensor Node: This node is represented by bright yellow color. It has the label is of the form `{tensor-name}{depth}: {tensor-shape}`. `tensor-name` can take 3 values input-tensor, hidden-tensor, or output-tensor. Depth is the depth of tensor in hierarchy of modules.  2\\) Function Node: This node is represented by bright blue color. Its label is of the form `{Function-name}{depth}: {input and output shape}`.  3\\) Module Node: This node is represented by bright green color. Its label is of the form `{Module-name}{depth}: {input and output shape}`.     In the example of MLP above, input tensor is called by main module MLP. This input tensor is called by its submodules, Sequential. Then, it is called by its submodule linear. Now, inside linear module exists linear function `F.linear`. This finally applied to input-tensor and returns output-tensor. This is sent to ReLU layer and so on.     Now, we show how rolling mechanism on recursive modules implemented. To demonstrate this, we use RNN module   <pre><code>class SimpleRNN(nn.Module):\n    \"\"\"Simple RNN\"\"\"\n\n    def __init__(self, inplace: bool = True) -&gt; None:\n        super().__init__()\n        self.hid_dim = 2\n        self.input_dim = 3\n        self.max_length = 4\n        self.lstm = nn.LSTMCell(self.input_dim, self.hid_dim)\n        self.activation = nn.LeakyReLU(inplace=inplace)\n        self.projection = nn.Linear(self.hid_dim, self.input_dim)\n\n    def forward(self, token_embedding: torch.Tensor) -&gt; torch.Tensor:\n        b_size = token_embedding.size()[0]\n        hx = torch.randn(b_size, self.hid_dim, device=token_embedding.device)\n        cx = torch.randn(b_size, self.hid_dim, device=token_embedding.device)\n\n        for _ in range(self.max_length):\n            hx, cx = self.lstm(token_embedding, (hx, cx))\n            hx = self.activation(hx)\n\n        return hx\n</code></pre> <pre><code>model_graph_2 = draw_graph(\n    SimpleRNN(), input_size=(2, 3),\n    graph_name='RecursiveNet',\n    roll=True\n)\n</code></pre> <pre><code>model_graph_2.visual_graph\n</code></pre>   In the graph above, we see a rolled representation of RNN with LSTM units. We see that LSTMCell and LeakyReLU nodes. This is representated by the numbers show on edges. These number near edges represent the number of edges that occur in `forward prop`. For instance, the first number 4 represent the number of times token_embedding is used.  If the number of times that edge is used is 1, then it is not shown.     Another useful feature is the resize feature. Say, the previous image of RNN is too big for your purpose. What we can do is to use resize feature rescale it by 0.5   <pre><code>model_graph_2.resize_graph(scale=0.5)\n</code></pre> <pre><code>model_graph_2.visual_graph\n</code></pre>   It got smaller !!!"},{"location":"zh/","title":"\u9996\u9875","text":""},{"location":"zh/#torchview","title":"torchview","text":"<p>Torchview \u7528\u4e8e\u5c06 PyTorch \u6a21\u578b\u53ef\u89c6\u5316\u4e3a\u8ba1\u7b97\u56fe\uff08visual graph\uff09\u3002\u53ef\u89c6\u5316\u5185\u5bb9\u5305\u62ec\uff1atensor\u3001module\u3001<code>torch</code> \u51fd\u6570\u8c03\u7528\uff0c\u4ee5\u53ca\u8f93\u5165/\u8f93\u51fa shape \u7b49\u4fe1\u606f\u3002</p> <p>\u5b83\u53ef\u4ee5\u7406\u89e3\u4e3a PyTorch \u7248\u672c\u7684 Keras <code>plot_model</code>\uff08\u5e76\u4e14\u652f\u6301\u66f4\u591a\u7ec6\u8282\uff09\u3002</p> <p>\u652f\u6301 PyTorch \u7248\u672c\uff1a(\\geq 1.7)\u3002</p>"},{"location":"zh/#_1","title":"\u4e3b\u8981\u7279\u6027","text":""},{"location":"zh/#_2","title":"\u5b89\u88c5","text":"<p>\u9996\u5148\u9700\u8981\u5b89\u88c5 Graphviz\uff1a</p> <pre><code>pip install graphviz\n</code></pre> <p>\u4e3a\u4e86\u8ba9 Graphviz \u7684 Python \u63a5\u53e3\u6b63\u5e38\u5de5\u4f5c\uff0c\u4f60\u7684\u7cfb\u7edf\u4e2d\u9700\u8981\u80fd\u591f\u8c03\u7528 <code>dot</code> \u5e03\u5c40\u547d\u4ee4\u3002\u5982\u679c\u5c1a\u672a\u5b89\u88c5 Graphviz\uff0c\u5efa\u8bae\u6309\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5\uff1a</p> <p>Debian \u7cfb Linux\uff08\u5982 Ubuntu\uff09\uff1a</p> <pre><code>apt-get install graphviz\n</code></pre> <p>Windows\uff1a</p> <pre><code>choco install graphviz\n</code></pre> <p>macOS\uff1a</p> <pre><code>brew install graphviz\n</code></pre> <p>\u66f4\u591a\u7ec6\u8282\u53ef\u53c2\u8003\uff1aGraphviz \u6587\u6863</p> <p>\u7136\u540e\u7528 pip \u5b89\u88c5 torchview\uff1a</p> <pre><code>pip install torchview\n</code></pre> <p>\u6216\u8005\u7528 conda\uff1a</p> <pre><code>conda install -c conda-forge torchview\n</code></pre> <p>\u5982\u679c\u4f60\u60f3\u5b89\u88c5\u6700\u65b0\u7248\u672c\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u4ed3\u5e93\u5b89\u88c5\uff1a</p> <pre><code>pip install git+https://github.com/mert-kurttutan/torchview.git\n</code></pre>"},{"location":"zh/#_3","title":"\u5feb\u901f\u4f7f\u7528","text":"<pre><code>from torchview import draw_graph\n\nmodel = MLP()\nbatch_size = 2\n# device='meta' -&gt; \u53ef\u89c6\u5316\u65f6\u4e0d\u4f1a\u6d88\u8017\u5b9e\u9645\u663e\u5b58/\u5185\u5b58\uff08\u53ea\u505a\u7ed3\u6784\u63a8\u5bfc\uff09\nmodel_graph = draw_graph(model, input_size=(batch_size, 128), device='meta')\nmodel_graph.visual_graph\n</code></pre>"},{"location":"zh/#notebook","title":"Notebook \u793a\u4f8b","text":"<p>\u66f4\u591a\u793a\u4f8b\u53ef\u53c2\u8003\u4e0b\u9762\u7684 Colab\uff1a</p> <p>\u5165\u95e8\u4ecb\u7ecd\uff1a </p> <p>\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff1a </p> <p>NLP \u6a21\u578b\uff1a </p> <p>\u6ce8\u610f\uff1a torchview \u7684 Graphviz \u53ef\u89c6\u5316\u4f1a\u8fd4\u56de\u201c\u9002\u914d\u5c3a\u5bf8\u201d\u7684\u56fe\u50cf\u3002\u4f46\u5728 VSCode \u4e2d\uff0c\u7531\u4e8e SVG \u6e32\u67d3\u4e0e\u753b\u5e03\u5c3a\u5bf8\u7684\u9650\u5236\uff0c\u8f83\u5927\u7684\u56fe\u53ef\u80fd\u51fa\u73b0\u88c1\u5207\u3002\u53ef\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6539\u7528 PNG \u6e32\u67d3\uff1a</p> <pre><code>import graphviz\ngraphviz.set_jupyter_format('png')\n</code></pre> <p>\u8be5\u95ee\u9898\u5728 JupyterLab\u3001Google Colab \u7b49\u5e73\u53f0\u901a\u5e38\u4e0d\u4f1a\u51fa\u73b0\u3002</p>"},{"location":"zh/#_4","title":"\u652f\u6301\u7684\u80fd\u529b","text":"<ul> <li>\u652f\u6301\u591a\u6570\u5e38\u89c1\u6a21\u578b\uff1aRNN\u3001<code>Sequential</code>\u3001\u8df3\u8fde\uff08Skip Connection\uff09\u3001Hugging Face \u6a21\u578b\u7b49</li> <li>\u652f\u6301 Meta Tensor\uff0c\u53ef\u5728\u53ef\u89c6\u5316\u8d85\u5927\u6a21\u578b\u65f6\u505a\u5230\u51e0\u4e4e\u4e0d\u6d88\u8017\u5185\u5b58\uff08PyTorch (\\geq 1.13)\uff09</li> <li>\u9664\u4e86 module \u8c03\u7528\uff0c\u8fd8\u80fd\u663e\u793a tensor \u4e4b\u95f4\u7684\u7b97\u5b50\u64cd\u4f5c</li> <li>\u652f\u6301 Rolling/Unrolling\uff1a\u53ef\u5c06\u9012\u5f52\u8c03\u7528\u7684\u6a21\u5757\u5728\u56fe\u4e0a\u201c\u6298\u53e0/\u5c55\u5f00\u201d\uff08\u89c1\u4e0b\u65b9\u793a\u4f8b\uff09</li> <li>\u652f\u6301\u591a\u79cd\u8f93\u5165/\u8f93\u51fa\u7c7b\u578b\uff1a\u5982\u5d4c\u5957\u7ed3\u6784\uff08dict/list \u7b49\uff09\u3001Hugging Face tokenizer \u8f93\u51fa\u7b49</li> </ul>"},{"location":"zh/#api","title":"API \u6587\u6863","text":"<pre><code>def draw_graph(\n    model: nn.Module,\n    input_data: INPUT_DATA_TYPE | None = None,\n    input_size: INPUT_SIZE_TYPE | None = None,\n    graph_name: str = 'model',\n    depth: int | float = 3,\n    device: torch.device | str | None = None,\n    dtypes: list[torch.dtype] | None = None,\n    mode: str | None = None,\n    strict: bool = True,\n    expand_nested: bool = False,\n    graph_dir: str | None = None,\n    hide_module_functions: bool = True,\n    hide_inner_tensors: bool = True,\n    roll: bool = False,\n    show_shapes: bool = True,\n    save_graph: bool = False,\n    filename: str | None = None,\n    directory: str = '.',\n    **kwargs: Any,\n) -&gt; ComputationGraph:\n    '''\u8fd4\u56de\u8f93\u5165 PyTorch Module \u7684\u53ef\u89c6\u5316\u8868\u793a\uff08ComputationGraph\uff09\u3002\n    ComputationGraph \u5305\u542b\uff1a\n\n    1) \u6839\u8282\u70b9\uff08\u901a\u5e38\u662f\u8f93\u5165 tensor \u8282\u70b9\uff09\uff0c\u5b83\u8fde\u63a5\u5230 forward \u8fc7\u7a0b\u4e2d\u8bb0\u5f55\u7684\u6240\u6709\u5176\u5b83\u8282\u70b9\n\n    2) `graphviz.Digraph` \u5bf9\u8c61\uff0c\u7528\u4e8e\u627f\u8f7d\u8ba1\u7b97\u56fe\u7684\u53ef\u89c6\u5316\u8868\u793a\u3002\u56fe\u4e2d\u4f1a\u5c55\u793a module/module \u5c42\u7ea7\u3001\n    torch \u51fd\u6570\u3001shape\uff0c\u4ee5\u53ca forward \u4e2d\u8bb0\u5f55\u5230\u7684 tensor\u3002\u76f8\u5173\u793a\u4f8b\u53ef\u89c1\u6587\u6863\u4e0e Colab notebook\u3002\n\n    Args:\n        model (nn.Module):\n            \u9700\u8981\u53ef\u89c6\u5316\u7684 PyTorch \u6a21\u578b\u3002\n\n        input_data (\u5305\u542b torch.Tensor \u7684\u6570\u636e\u7ed3\u6784):\n            \u4f5c\u4e3a\u6a21\u578b forward \u7684\u8f93\u5165\u3002\u591a\u4e2a\u4f4d\u7f6e\u53c2\u6570\u53ef\u653e\u5165 list\uff1b\n            \u6216\u7528 dict / kwargs \u5f62\u5f0f\u4f20\u5165\u3002\n\n        input_size (shape \u5e8f\u5217):\n            \u8f93\u5165\u6570\u636e\u7684 shape\uff08list/tuple/torch.Size\uff09\u3002\u5982\u679c\u7ed9\u4e86 `input_size`\uff0c\n            \u90a3\u4e48 `dtypes` \u9700\u8981\u4e0e\u6a21\u578b\u8f93\u5165\u4e00\u81f4\uff08\u9ed8\u8ba4\u4f7f\u7528 FloatTensor\uff09\u3002\n            Default: None\n\n        graph_name (str):\n            Graphviz `Digraph` \u7684\u540d\u79f0\uff0c\u4e5f\u4f1a\u4f5c\u4e3a\u9ed8\u8ba4\u8f93\u51fa\u6587\u4ef6\u540d\u3002\n            Default: 'model'\n\n        depth (int):\n            \u53ef\u89c6\u5316\u4e2d\u8282\u70b9\u5c55\u793a\u7684\u6700\u5927\u6df1\u5ea6\u3002\u6df1\u5ea6\u5b9a\u4e49\u4e3a\uff1a\u8282\u70b9\u5728\u6a21\u5757\u5c42\u7ea7\u4e2d\u7684\u201c\u5d4c\u5957\u5c42\u6570\u201d\u3002\n            \u4f8b\u5982\u4e3b\u6a21\u5757 depth=0\uff0c\u4e3b\u6a21\u5757\u7684\u5b50\u6a21\u5757 depth=1\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002\n            Default: 3\n\n        device (str or torch.device):\n            \u653e\u7f6e\u8f93\u5165 tensor \u7684 device\u3002\u82e5\u672a\u6307\u5b9a\uff1a\n            - PyTorch \u68c0\u6d4b\u5230 CUDA \u5219\u4f7f\u7528 GPU\n            - \u5426\u5219\u4f7f\u7528 CPU\n            Default: None\n\n        dtypes (list[torch.dtype]):\n            \u5f53\u63d0\u4f9b `input_size` \u65f6\uff0c\u7528 `dtypes` \u8bbe\u7f6e\u8f93\u5165 tensor \u7684 dtype\u3002\n\n        mode (str):\n            forward \u4f20\u64ad\u65f6\u4f7f\u7528\u7684\u6a21\u578b\u6a21\u5f0f\uff1b\u672a\u6307\u5b9a\u5219\u9ed8\u8ba4\u7528 eval\u3002\n            Default: None\n\n        strict (bool):\n            \u5982\u679c\u4e3a true\uff0c\u5219 Graphviz \u53ef\u89c6\u5316\u4e0d\u5141\u8bb8\u540c\u4e00\u5bf9\u8282\u70b9\u4e4b\u95f4\u51fa\u73b0\u591a\u6761\u8fb9\u3002\n            \u591a\u6761\u8fb9\u53ef\u80fd\u53d1\u751f\u5728\uff1amodule \u8282\u70b9\u4e4b\u95f4\u540c\u65f6\u5b58\u5728 tensor \u8fb9\uff0c\u4f46\u4f60\u53c8\u9009\u62e9\u9690\u85cf\u8fd9\u4e9b tensor \u65f6\u3002\n            Default: True\n\n        expand_nested(bool):\n            \u5982\u679c\u4e3a true\uff0c\u5219\u7528\u865a\u7ebf\u8fb9\u6846\u5c55\u793a\u5d4c\u5957\u6a21\u5757\u3002\n\n        graph_dir (str):\n            \u8bbe\u7f6e\u56fe\u7684\u65b9\u5411\uff1a\n            'TB' -&gt; \u4ece\u4e0a\u5230\u4e0b\n            'LR' -&gt; \u4ece\u5de6\u5230\u53f3\n            'BT' -&gt; \u4ece\u4e0b\u5230\u4e0a\n            'RL' -&gt; \u4ece\u53f3\u5230\u5de6\n            Default: None -&gt; TB\n\n        hide_module_function (bool):\n            \u662f\u5426\u9690\u85cf module \u5185\u90e8\u7684 torch function\u3002\u90e8\u5206\u6a21\u5757\u53ea\u7531 torch function \u6784\u6210\uff08\u65e0\u5b50\u6a21\u5757\uff09\uff0c\n            \u4f8b\u5982 `nn.Conv2d`\u3002\n            True =&gt; \u4e0d\u5728\u56fe\u4e2d\u5c55\u793a module functions\n            False =&gt; \u5728\u56fe\u4e2d\u5c55\u793a module functions\n            Default: True\n\n        hide_inner_tensors (bool):\n            inner tensor \u6307\u9664\u8f93\u5165/\u8f93\u51fa\u5916\uff0c\u5728\u8ba1\u7b97\u56fe\u5185\u90e8\u6d41\u8f6c\u7684 tensor\u3002\n            True =&gt; \u4e0d\u5c55\u793a inner tensors\n            False =&gt; \u5c55\u793a inner tensors\n            Default: True\n\n        roll (bool):\n            \u82e5\u4e3a true\uff0c\u5219\u6298\u53e0\u9012\u5f52\u6a21\u5757\uff08Rolling\uff09\u3002\n            Default: False\n\n        show_shapes (bool):\n            True =&gt; \u5c55\u793a tensor \u7684 shape\uff08\u542b\u8f93\u5165/\u8f93\u51fa\uff09\n            False =&gt; \u4e0d\u5c55\u793a shape\n            Default: True\n\n        save_graph (bool):\n            True =&gt; \u4fdd\u5b58 Graphviz \u8f93\u51fa\u6587\u4ef6\n            False =&gt; \u4e0d\u4fdd\u5b58\n            Default: False\n\n        filename (str):\n            \u4fdd\u5b58 dot \u8bed\u6cd5\u4e0e\u56fe\u50cf\u6587\u4ef6\u65f6\u4f7f\u7528\u7684\u6587\u4ef6\u540d\uff1b\u9ed8\u8ba4\u7b49\u4e8e graph_name\u3002\n\n        directory (str):\n            \u4fdd\u5b58 Graphviz \u8f93\u51fa\u6587\u4ef6\u7684\u76ee\u5f55\u3002\n            Default: .\n\n    Returns:\n        ComputationGraph\uff1a\u5305\u542b Graphviz `Digraph` \u7684\u8ba1\u7b97\u56fe\u5bf9\u8c61\u3002\n    '''\n</code></pre>"},{"location":"zh/#_5","title":"\u793a\u4f8b","text":""},{"location":"zh/#rolled-version","title":"\u9012\u5f52\u7f51\u7edc\u7684\u6298\u53e0\uff08Rolled Version\uff09","text":"<pre><code>from torchview import draw_graph\n\nmodel_graph = draw_graph(\n    SimpleRNN(), input_size=(2, 3),\n    graph_name='RecursiveNet',\n    roll=True\n)\nmodel_graph.visual_graph\n</code></pre>"},{"location":"zh/#hiddentensor-functionals","title":"\u663e\u793a/\u9690\u85cf\u4e2d\u95f4\uff08hidden\uff09tensor \u4e0e functionals","text":"<pre><code># Show inner tensors and Functionals\nmodel_graph = draw_graph(\n    MLP(), input_size=(2, 128),\n    graph_name='MLP',\n    hide_inner_tensors=False,\n    hide_module_functions=False,\n)\n\nmodel_graph.visual_graph\n</code></pre>"},{"location":"zh/#resnet-torch","title":"ResNet / \u8df3\u8fde / \u652f\u6301 torch \u8fd0\u7b97 / \u5d4c\u5957\u6a21\u5757\u5c55\u793a","text":"<pre><code>import torchvision\n\nmodel_graph = draw_graph(resnet18(), input_size=(1,3,32,32), expand_nested=True)\nmodel_graph.visual_graph\n</code></pre>"},{"location":"zh/#todo","title":"TODO","text":"<ul> <li>[ ] \u5c55\u793a Module \u7684\u53c2\u6570\u4fe1\u606f\uff08parameter info\uff09</li> <li>[ ] \u652f\u6301\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09</li> <li>[ ] \u4e3a GNN \u652f\u6301\u65e0\u5411\u8fb9</li> <li>[ ] \u652f\u6301 torch-based functions[^1]</li> </ul> <p>[^1]: \u8fd9\u91cc\u7684 torch-based functions \u6307\u201c\u53ea\u4f7f\u7528 torch \u51fd\u6570\u548c\u6a21\u5757\u5b9e\u73b0\u7684\u51fd\u6570\u201d\u3002\u8be5\u6982\u5ff5\u6bd4 module \u66f4\u6cdb\u5316\u3002</p>"},{"location":"zh/#_6","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u4e2d\u6587\u7248\u672c\u7531 @1985312383\uff08GitHub\uff09\u53cb\u60c5\u63d0\u4f9b\u3002</p> <p>\u6211\u4eec\u975e\u5e38\u6b22\u8fce issue \u4e0e PR\uff01\u5982\u679c\u4f60\u60f3\u4e86\u89e3\u5982\u4f55\u6784\u5efa\u672c\u9879\u76ee\uff1a</p> <ul> <li>torchview \u4f7f\u7528\u6700\u65b0 Python \u7248\u672c\u8fdb\u884c\u6d3b\u8dc3\u5f00\u53d1\u3002</li> <li>\u6539\u52a8\u9700\u8981\u4fdd\u6301\u5bf9 Python 3.7 \u7684\u5411\u540e\u517c\u5bb9\uff0c\u5e76\u9075\u5faa Python \u7684\u65e7\u7248\u672c\u751f\u547d\u5468\u671f\u7b56\u7565\u3002</li> <li>\u8fd0\u884c <code>pip install -r requirements-dev.txt</code> \u5b89\u88c5\u5f00\u53d1\u4f9d\u8d56\uff08\u6211\u4eec\u4f7f\u7528\u6700\u65b0\u7684 dev \u5305\u7248\u672c\uff09\u3002</li> <li>\u5355\u6d4b\uff1a\u8fd0\u884c <code>pytest</code></li> <li>\u66f4\u65b0\u671f\u671b\u8f93\u51fa\uff1a\u8fd0\u884c <code>pytest --overwrite</code></li> <li>\u8df3\u8fc7\u8f93\u51fa\u6587\u4ef6\u6d4b\u8bd5\uff1a\u8fd0\u884c <code>pytest --no-output</code></li> </ul>"},{"location":"zh/#_7","title":"\u53c2\u8003","text":"<ul> <li>\u8f93\u5165\u5904\u7406\u4e0e\u6821\u9a8c\u76f8\u5173\u90e8\u5206\u501f\u9274/\u53c2\u8003\u4e86 torchinfo \u4ed3\u5e93</li> <li>\u8f6f\u4ef6\u5de5\u7a0b\u76f8\u5173\u90e8\u5206\uff08\u5982\u6d4b\u8bd5\uff09\u4e5f\u501f\u9274\u4e86 torchinfo\uff08\u611f\u8c22 @TylerYep\uff09</li> <li>\u8ba1\u7b97\u56fe\u6784\u5efa\u7b97\u6cd5\u5f97\u76ca\u4e8e <code>__torch_function__</code> \u4e0e <code>torch.Tensor</code> \u7684 subclass \u673a\u5236\uff08\u611f\u8c22\u76f8\u5173\u8d21\u732e\u8005\uff09</li> </ul>"},{"location":"zh/model_gallery/","title":"\u6a21\u578b\u753b\u5eca","text":""},{"location":"zh/model_gallery/#model-gallery","title":"\u6a21\u578b\u753b\u5eca (Model Gallery)","text":"<p>\u672c\u9875\u5c55\u793a\u4e86\u4f7f\u7528 <code>torchview</code> \u751f\u6210\u7684\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u89c6\u5316\u7ed3\u679c\u3002</p> <p>\u6240\u6709\u56fe\u7247\u5747\u4e3a SVG \u77e2\u91cf\u56fe\uff0c\u60a8\u53ef\u4ee5\u70b9\u51fb\u56fe\u7247\u6216\u5728\u6d4f\u89c8\u5668\u4e2d\u653e\u5927\u4ee5\u67e5\u770b\u6240\u6709\u7ec6\u8282\u3002</p>"},{"location":"zh/model_gallery/#computer-vision","title":"\ud83d\uddbc\ufe0f \u8ba1\u7b97\u673a\u89c6\u89c9 (Computer Vision)","text":""},{"location":"zh/model_gallery/#resnet-50","title":"ResNet-50","text":"<p>\u6b8b\u5dee\u7f51\u7edc\uff1a\u7ecf\u5178\u7684 CNN \u67b6\u6784\uff0c\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\uff08Skip Connections\uff09\u89e3\u51b3\u4e86\u6df1\u5c42\u7f51\u7edc\u7684\u8bad\u7ec3\u96be\u9898\u3002\u56fe\u4e2d\u6e05\u6670\u5c55\u793a\u4e86 Bottleneck \u7ed3\u6784\u3002</p>"},{"location":"zh/model_gallery/#inception-v3","title":"Inception V3","text":"<p>Inception \u67b6\u6784\uff1a\u4ee5\u591a\u5c3a\u5ea6\u5206\u652f\uff08Inception Module\uff09\u8457\u79f0\uff0c\u7ed3\u6784\u590d\u6742\u4f46\u9ad8\u6548\u3002</p>"},{"location":"zh/model_gallery/#vision-transformer-vit","title":"Vision Transformer (ViT)","text":"<p>ViT\uff1a\u5c06 Transformer \u67b6\u6784\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u50cf\u5757\uff08Patches\uff09\uff0c\u5b8c\u5168\u6452\u5f03\u4e86\u5377\u79ef\u64cd\u4f5c\u3002</p>"},{"location":"zh/model_gallery/#fcn-fully-convolutional-network","title":"FCN (Fully Convolutional Network)","text":"<p>\u5168\u5377\u79ef\u7f51\u7edc\uff1a\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u7ecf\u5178\u6a21\u578b\uff0c\u57fa\u4e8e ResNet-50 \u4e3b\u5e72\u3002</p>"},{"location":"zh/model_gallery/#nlp","title":"\ud83d\udcdd \u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP)","text":""},{"location":"zh/model_gallery/#transformer-encoder-bert-style","title":"Transformer Encoder (BERT-style)","text":"<p>Transformer \u7f16\u7801\u5668\uff1a\u5305\u542b Multi-Head Attention \u548c Feed Forward Network\uff0c\u662f BERT \u7b49\u6a21\u578b\u7684\u57fa\u7840\u3002</p>"},{"location":"zh/model_gallery/#seq2seq-with-attention","title":"Seq2Seq with Attention","text":"<p>\u5e8f\u5217\u5230\u5e8f\u5217 + \u6ce8\u610f\u529b\u673a\u5236\uff1a\u7ecf\u5178\u7684\u673a\u5668\u7ffb\u8bd1\u67b6\u6784\u3002\u56fe\u4e2d\u5c55\u793a\u4e86 Encoder\uff08\u5de6\uff09\u548c Decoder\uff08\u53f3\uff09\u4ee5\u53ca\u4e2d\u95f4\u7684 Attention \u8ba1\u7b97\u8fc7\u7a0b\uff08\u6a2a\u5411\u5e03\u5c40\uff09\u3002</p>"},{"location":"zh/model_gallery/#recommender-systems","title":"\ud83d\uded2 \u63a8\u8350\u7cfb\u7edf (Recommender Systems)","text":""},{"location":"zh/model_gallery/#wide-deep","title":"Wide &amp; Deep","text":"<p>Wide &amp; Deep\uff1a\u7ed3\u5408\u4e86\u7ebf\u6027\u6a21\u578b\uff08Wide\uff0c\u8bb0\u5fc6\u80fd\u529b\uff09\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08Deep\uff0c\u6cdb\u5316\u80fd\u529b\uff09\u7684\u7ecf\u5178\u63a8\u8350\u67b6\u6784\u3002</p>"},{"location":"zh/model_gallery/#generative-models","title":"\ud83c\udfa8 \u751f\u6210\u6a21\u578b (Generative Models)","text":""},{"location":"zh/model_gallery/#vae-variational-autoencoder","title":"VAE (Variational Autoencoder)","text":"<p>\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff1a\u56fe\u4e2d\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u751f\u6210\u6d41\u7a0b\uff1aEncoder \u2192 Reparameterization (\u91c7\u6837) \u2192 Decoder\u3002</p>"},{"location":"zh/model_gallery/#special-architectures","title":"\u2b50 \u7279\u6b8a\u67b6\u6784 (Special Architectures)","text":""},{"location":"zh/model_gallery/#wavenet","title":"WaveNet","text":"<p>WaveNet\uff1a\u7528\u4e8e\u8bed\u97f3\u5408\u6210\u7684\u751f\u6210\u6a21\u578b\u3002\u56fe\u4e2d\u6e05\u6670\u5c55\u793a\u4e86\u57fa\u4e8e\u7a7a\u6d1e\u5377\u79ef\uff08Dilated Convolution\uff09\u7684\u6b8b\u5dee\u5757\u5806\u53e0\u7ed3\u6784\uff08\u6a2a\u5411\u5e03\u5c40\uff09\u3002</p>"},{"location":"zh/model_gallery/#gcn-graph-convolutional-network","title":"GCN (Graph Convolutional Network)","text":"<p>\u56fe\u5377\u79ef\u7f51\u7edc\uff1a\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u7684\u795e\u7ecf\u7f51\u7edc\u3002</p>"},{"location":"zh/model_gallery/#_1","title":"\u5982\u4f55\u8d21\u732e","text":"<p>\u5982\u679c\u60a8\u60f3\u4e3a\u753b\u5eca\u6dfb\u52a0\u65b0\u7684\u6a21\u578b\uff0c\u8bf7\u786e\u4fdd\uff1a 1. \u6a21\u578b\u7ed3\u6784\u5177\u6709\u4ee3\u8868\u6027\u3002 2. \u4f7f\u7528 <code>torchview</code> \u751f\u6210 SVG \u683c\u5f0f\u4ee5\u4fdd\u8bc1\u6e05\u6670\u5ea6\u3002 3. \u63d0\u4ea4 PR \u5230\u6211\u4eec\u7684\u4ed3\u5e93\u3002</p>"},{"location":"zh/used_by/","title":"\u751f\u6001","text":""},{"location":"zh/used_by/#used-by","title":"\u751f\u6001 / Used By","text":"<ul> <li> <p>Torch-RecHub\uff08Datawhale\uff09     ---     \u4e00\u4e2a\u57fa\u4e8e PyTorch \u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u5f3a\u8c03\u6807\u51c6\u5316\u8bad\u7ec3/\u8bc4\u6d4b\u6d41\u7a0b\u4e0e\u53ef\u590d\u73b0\u5b9e\u9a8c\u3002</p> <p>\u4eae\u70b9\uff1a</p> <ul> <li>\u8986\u76d6 30+ \u4e3b\u6d41\u63a8\u8350\u6a21\u578b\uff08\u6392\u5e8f / \u53ec\u56de / \u591a\u4efb\u52a1 / \u751f\u6210\u5f0f\uff09</li> <li>\u6a21\u5757\u5316\u8bbe\u8ba1\uff1a\u4fbf\u4e8e\u6269\u5c55\u6a21\u578b\u3001\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u6307\u6807</li> <li>\u7edf\u4e00\u7684\u6570\u636e\u52a0\u8f7d\u3001\u8bad\u7ec3\u3001\u8bc4\u4f30\u6d41\u7a0b</li> <li>\u652f\u6301\u5bfc\u51fa ONNX\uff0c\u4fbf\u4e8e\u90e8\u7f72</li> </ul> <p>GitHub</p> </li> <li> <p>\u6dfb\u52a0\u4f60\u7684\u9879\u76ee     ---     \u6b22\u8fce\u63d0\u4ea4 PR \u65b0\u589e\u4e00\u5f20\u5361\u7247\uff0c\u5305\u542b\uff1a</p> <ul> <li>\u9879\u76ee\u540d\u79f0</li> <li>\u4e00\u53e5\u8bdd\u7b80\u4ecb</li> <li>\u4ed3\u5e93 / \u5b98\u7f51\u94fe\u63a5</li> </ul> </li> </ul>"},{"location":"zh/developer/","title":"\u6982\u89c8","text":""},{"location":"zh/developer/#_1","title":"\u5f00\u53d1\u8005\u6587\u6863","text":"<p>\u672c\u90e8\u5206\u9762\u5411\u5e0c\u671b\u4e86\u89e3 torchview \u5185\u90e8\u5b9e\u73b0\u673a\u5236\u7684\u5f00\u53d1\u8005\uff1a\u5305\u62ec\u8ba1\u7b97\u56fe\u6784\u5efa\u3001rolling\uff08\u9012\u5f52\u6298\u53e0\uff09\u7b56\u7565\uff0c\u4ee5\u53ca <code>__torch_function__</code> \u76f8\u5173\u6ce8\u610f\u4e8b\u9879\u7b49\u3002</p>"},{"location":"zh/developer/docs/","title":"\u6587\u6863\uff08MkDocs\uff09","text":""},{"location":"zh/developer/docs/#mkdocs","title":"\u6587\u6863\uff08MkDocs\uff09","text":"<p>\u8fd9\u662f TorchView \u9879\u76ee\u7684\u6587\u6863\u8bf4\u660e\u3002\u6587\u6863\u7ad9\u70b9\u57fa\u4e8e MkDocs \u6784\u5efa\u3002</p>"},{"location":"zh/developer/docs/#_1","title":"\u5e38\u7528\u547d\u4ee4","text":"<ul> <li><code>mkdocs new [dir-name]</code>\uff1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 MkDocs \u9879\u76ee</li> <li><code>mkdocs serve</code>\uff1a\u542f\u52a8\u672c\u5730\u9884\u89c8\uff08\u652f\u6301\u70ed\u66f4\u65b0\uff09</li> <li><code>mkdocs build</code>\uff1a\u6784\u5efa\u9759\u6001\u7ad9\u70b9</li> <li><code>mkdocs -h</code>\uff1a\u67e5\u770b\u5e2e\u52a9\u5e76\u9000\u51fa</li> </ul>"},{"location":"zh/developer/docs/#_2","title":"\u9879\u76ee\u7ed3\u6784","text":"<pre><code>mkdocs.yml    # \u914d\u7f6e\u6587\u4ef6\ndocs/\n    index.md  # \u6587\u6863\u4e3b\u9875\n    ...       # \u5176\u4ed6 Markdown \u9875\u9762\u3001\u56fe\u7247\u7b49\u8d44\u6e90\n</code></pre>"},{"location":"zh/developer/rolling_graph/","title":"\u9012\u5f52\u56fe\u6298\u53e0\uff08Rolling Graph\uff09","text":""},{"location":"zh/developer/rolling_graph/#rolling","title":"\u5982\u4f55\u8bc6\u522b\u8282\u70b9\u5e76\u6298\u53e0\u9012\u5f52\u56fe\uff08Rolling\uff09\uff1f","text":""},{"location":"zh/developer/rolling_graph/#todo","title":"TODO\uff08\u539f\u6587\u4fdd\u7559\uff09","text":"<p>TODO: \u6dfb\u52a0\u56fe\u7247\u4e0e\u66f4\u6e05\u6670\u7684\u89e3\u91ca</p>"},{"location":"zh/developer/rolling_graph/#_1","title":"\u6982\u8ff0","text":"<p>\u672c\u6587\u6863\u63cf\u8ff0 torchview \u5728 rolling\uff08\u9012\u5f52\u6298\u53e0\uff09 \u573a\u666f\u4e0b\uff0c\u5982\u4f55\u8bc6\u522b\u8ba1\u7b97\u8282\u70b9\u201c\u662f\u5426\u5e94\u5f53\u88ab\u89c6\u4e3a\u540c\u4e00\u4e2a\u8282\u70b9\u201d\uff0c\u4ee5\u53ca\u6700\u7ec8\u5982\u4f55\u5448\u73b0\u5728\u53ef\u89c6\u5316\u56fe\u4e2d\u3002</p> <p>\u9996\u5148\u9700\u8981\u8bf4\u660e\uff1a\u7528\u4e8e\u8bc6\u522b\u8282\u70b9\u7684\u6838\u5fc3\u4fe1\u606f\u662f <code>ComputationNode</code> \u7684 <code>node_id</code> \u5c5e\u6027\u3002\u4e3e\u4f8b\u6765\u8bf4\uff0c\u5982\u679c\u4e24\u4e2a\u8282\u70b9\uff08\u901a\u5e38\u6765\u81ea\u9012\u5f52\u91cd\u590d\u4f7f\u7528\uff09\u62e5\u6709\u76f8\u540c\u7684 <code>node_id</code>\uff0c\u5b83\u4eec\u4f1a\u5728\u6700\u7ec8\u7684\u53ef\u89c6\u5316\u56fe\u4e2d\u5408\u5e76\u4e3a\u540c\u4e00\u4e2a\u8282\u70b9\u3002</p>"},{"location":"zh/developer/rolling_graph/#rolling_1","title":"Rolling \u7684\u4e24\u79cd\u6a21\u5f0f","text":"<p>rolling \u673a\u5236\u4e3b\u8981\u53ea\u6709\u4e24\u79cd\u6a21\u5f0f\uff1a</p> <ul> <li><code>roll=False</code></li> <li><code>roll=True</code></li> </ul>"},{"location":"zh/developer/rolling_graph/#rollfalse","title":"<code>roll=False</code>","text":"<p>\u5f53 <code>roll=False</code> \u65f6\uff0c\u6bcf\u4e00\u6b21\u8ba1\u7b97\u6b65\u9aa4\u90fd\u4f1a\u5728\u56fe\u4e2d\u552f\u4e00\u5c55\u793a\uff08\u4e0d\u7ba1\u5b83\u662f\u5426\u6765\u81ea\u9012\u5f52\u590d\u7528\uff09\u3002\u5b9e\u73b0\u65b9\u5f0f\u5f88\u7b80\u5355\uff1a\u5c06\u6bcf\u4e2a\u8282\u70b9\u7684 <code>node_id</code> \u8bbe\u4e3a\u8be5 <code>ComputationNode</code> Python \u5bf9\u8c61\u7684 id\uff08\u4f8b\u5982 <code>node_id = id(tensor_node_1)</code>\uff09\u3002</p>"},{"location":"zh/developer/rolling_graph/#rolltrue","title":"<code>roll=True</code>","text":"<p>\u5f53 <code>roll=True</code> \u65f6\uff0c\u6211\u4eec\u4f1a\u5c06\u201c\u9012\u5f52\u590d\u7528\u7684\u6a21\u5757\u201d\u5728\u53ef\u89c6\u5316\u56fe\u91cc\u8bc6\u522b\u4e3a\u540c\u4e00\u4e2a\u8282\u70b9\u3002\u5177\u4f53\u5982\u4f55\u8bc6\u522b\uff0c\u53d6\u51b3\u4e8e\u8282\u70b9\u7c7b\u578b\uff1a</p> <ol> <li>TensorNode</li> </ol> <p>TensorNode \u4e0d\u4f1a\u88ab\u6298\u53e0\uff0c\u6bcf\u4e2a tensor \u90fd\u4f1a\u88ab\u552f\u4e00\u8bc6\u522b\u3002\u8fd9\u662f\u56e0\u4e3a tensor \u901a\u5e38\u4e0d\u50cf\u9012\u5f52\u6a21\u5757\u90a3\u6837\u201c\u91cd\u590d\u4f7f\u7528\u540c\u4e00\u4e2a\u5bf9\u8c61\u201d\uff0c\u5e76\u4e14\u628a tensor \u6298\u53e0\u4f1a\u8ba9\u56fe\u66f4\u96be\u7406\u89e3\u3002</p> <ol> <li>FunctionNode</li> </ol> <p>FunctionNode \u7684\u8bc6\u522b\u57fa\u4e8e\uff1a\u4e0e\u8be5 FunctionNode \u7684 \u8f93\u51fa\u76f8\u5173\u8054\u7684 torch \u51fd\u6570\u7684 id\u3002\u5982\u679c\u8fd9\u4e9b\u76f8\u540c\uff0c\u5219\u4e24\u4e2a FunctionNode \u4f1a\u88ab\u89c6\u4e3a\u540c\u4e00\u4e2a\u3002</p> <p>\u4e3a\u4ec0\u4e48\u8981\u7528\u201c\u8f93\u51fa\u201d\u6765\u8bc6\u522b\uff1f\u539f\u56e0\u53ef\u4ee5\u5206\u51e0\u79cd\u8ba8\u8bba\uff1a</p> <ul> <li>\u4e0d\u80fd\u53ea\u7528 torch \u51fd\u6570\u81ea\u8eab\u7684 id\uff1a\u4f8b\u5982\u4e24\u5904\u4e0d\u540c\u4f4d\u7f6e\u90fd\u8c03\u7528\u4e86 <code>torch.add</code>\uff08\u4e0d\u540c\u8df3\u8fde\uff09\uff0c\u5982\u679c\u53ea\u6309\u51fd\u6570 id \u5408\u5e76\uff0c\u4f1a\u628a\u76f8\u8ddd\u5f88\u8fdc\u7684\u7ed3\u6784\u5408\u5e76\u5230\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u89c6\u89c9\u6548\u679c\u5f88\u5dee\u3002</li> <li>\u7528\u8f93\u5165\u6765\u8bc6\u522b\u4e5f\u4e0d\u7406\u60f3\uff1a\u8bbe\u60f3\u540c\u4e00\u4e2a\u8f93\u5165 tensor \u5206\u522b\u8d70\u5411\u4e24\u4e2a\u4e0d\u540c\u7684 <code>torch.relu</code> \u8c03\u7528\u3002\u6211\u4eec\u5e0c\u671b\u56fe\u4e2d\u770b\u5230\u4e24\u4e2a\u5206\u652f\u3002\u4f46\u5982\u679c\u6309\u8f93\u5165\u8bc6\u522b\uff0c\u4f1a\u5bfc\u81f4\u53ea\u51fa\u73b0\u4e00\u4e2a\u5206\u652f\uff08\u56e0\u4e3a\u8f93\u5165\u76f8\u540c\uff09\u3002</li> <li>\u7528\u8f93\u51fa\u8bc6\u522b\u66f4\u5408\u7406\uff1a\u6bcf\u4e2a\u8f93\u51fa tensor \u90fd\u6765\u81ea\u4e00\u6b21\u552f\u4e00\u7684\u51fd\u6570\u8c03\u7528\u3002\u6362\u8a00\u4e4b\uff0c\u4e00\u65e6\u786e\u5b9a\u4e86\u8f93\u51fa tensor\uff0c\u5c31\u80fd\u552f\u4e00\u786e\u5b9a\u5b83\u5bf9\u5e94\u7684 FunctionNode\u3002\u6240\u4ee5\u4e0a\u8ff0 <code>torch.relu</code> \u7684\u4f8b\u5b50\u4f1a\u4ea7\u751f\u4e24\u4e2a\u72ec\u7acb\u5206\u652f\uff0c\u7b26\u5408\u76f4\u89c9\u3002</li> </ul> <p>\u6b64\u5916\uff0c\u5f53\u6211\u4eec\u9690\u85cf inner tensors \u65f6\uff0c\u7528\u6765\u8bc6\u522b\u7684\u201c\u8f93\u51fa\u201d\u4e0d\u4e00\u5b9a\u662f TensorNode\u3002\u4e0a\u9762\u7684\u8bba\u8bc1\u540c\u6837\u9002\u7528\u4e8e\u8f93\u51fa\u4e3a ModuleNode \u6216 FunctionNode \u7684\u60c5\u51b5\uff1a\u8f93\u5165\u8bc6\u522b\u4e0d\u53ef\u9760\uff0c\u800c\u8f93\u51fa\u8bc6\u522b\u80fd\u552f\u4e00\u786e\u5b9a\u5bf9\u5e94\u7684\u8ba1\u7b97\u8282\u70b9\u3002</p> <ol> <li>ModuleNode</li> </ol> <p>ModuleNode \u5206\u4e3a\u4e24\u79cd\u60c5\u51b5\uff1a\u65e0\u72b6\u6001\u6a21\u5757\uff08Stateless\uff09 \u4e0e \u6709\u72b6\u6001\u6a21\u5757\uff08Stateful\uff09\u3002</p> <ul> <li> <p>\u65e0\u72b6\u6001\u6a21\u5757\uff08Stateless Module\uff09\uff1a\u6307\u4e0d\u5305\u542b <code>torch.nn.parameter.Parameter</code> \u7684\u6a21\u5757\u3002\u5b83\u4eec\u901a\u5e38\u53ea\u521b\u5efa\u4e00\u6b21\uff0c\u4f46\u4f1a\u5728\u4e0d\u540c\u4f4d\u7f6e\u88ab\u91cd\u590d\u8c03\u7528\u591a\u6b21\uff0c\u56e0\u6b64\u884c\u4e3a\u66f4\u50cf\u201c\u51fd\u6570\u201d\u3002\u6240\u4ee5\u5b83\u4eec\u7684\u8bc6\u522b\u65b9\u5f0f\u4e0e FunctionNode \u76f8\u540c\uff08\u6309\u201c\u8f93\u51fa\u5173\u8054\u4fe1\u606f\u201d\u6765\u8bc6\u522b\uff09\u3002\u5426\u5219\u5982\u679c\u6309 Python \u5bf9\u8c61 id \u8bc6\u522b\uff0c\u6240\u6709 ReLU \u4e4b\u7c7b\u7684\u6a21\u5757\u53ef\u80fd\u4f1a\u88ab\u8fde\u5230\u540c\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u56fe\u4f1a\u975e\u5e38\u96be\u770b\u3002</p> </li> <li> <p>\u6709\u72b6\u6001\u6a21\u5757\uff08Stateful Module\uff09\uff1a\u8fd9\u7c7b\u6a21\u5757\u6309 Python \u5bf9\u8c61\uff08ModuleNode\uff09\u7684 id \u6765\u8bc6\u522b\u3002\u8fd9\u4e5f\u5f88\u81ea\u7136\uff1a\u5f53\u6211\u4eec\u8fdb\u884c rolling \u6298\u53e0\u65f6\uff0c\u5e0c\u671b\u201c\u62e5\u6709\u76f8\u540c\u53c2\u6570\u7684\u540c\u4e00\u4e2a\u6a21\u5757\u5bf9\u8c61\u201d\u5728\u56fe\u4e2d\u5448\u73b0\u4e3a\u540c\u4e00\u4e2a\u8282\u70b9\u3002</p> </li> </ul>"},{"location":"zh/developer/torch_function_notes/","title":"__torch_function__ \u8bf4\u660e","text":""},{"location":"zh/developer/torch_function_notes/#__torch_function__","title":"<code>__torch_function__</code>","text":"<p>\u672c\u6587\u6863\u89e3\u91ca <code>__torch_function__</code> \u5728\u672c\u9879\u76ee\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u82e5\u5e72\u4e0e\u7248\u672c\u76f8\u5173\u7684\u6ce8\u610f\u4e8b\u9879\u3002</p> <ul> <li> <p><code>__torch_function__</code> API \u6700\u65e9\u5728 v1.5.0 \u5f15\u5165\uff0c\u89c1\uff1aPyTorch v1.5.0 Release Notes</p> </li> <li> <p>\u4e00\u4e9b\u91cd\u8981\u7b97\u5b50\u5bf9 subclass \u7684\u4fdd\u7559\uff08subclass preservation\uff09\u5728 v1.7.0 \u5f15\u5165\uff0c\u89c1\uff1aPyTorch v1.7.0 Release Notes\u3002\u8fd9\u4e00\u70b9\u975e\u5e38\u5173\u952e\uff0c\u56e0\u4e3a <code>torchview</code> \u53ea\u8ffd\u8e2a <code>RecorderTensor</code>\uff08<code>torch.Tensor</code> \u7684\u5b50\u7c7b\uff09\u4ea7\u751f\u7684 tensor \u6d41\u3002</p> </li> <li> <p>v1.9.0 \u5f15\u5165\u4e86\u4e00\u4e9b\u91cd\u8981\u4fee\u590d\uff0c\u4f8b\u5982\u5bf9 <code>F.embedding</code> \u7684\u652f\u6301\uff0c\u89c1\uff1aPyTorch v1.9.0 Release Notes\u3002\u5728\u66f4\u65e9\u7248\u672c\u4e2d\uff0csubclass \u7684 <code>__torch_function__</code> \u4e0b\u8c03\u7528 <code>F.embedding</code> \u53ef\u80fd\u8fd4\u56de <code>NotImplemented</code>\uff0c\u5bfc\u81f4\u8fd4\u56de\u503c\u9000\u5316\u4e3a\u666e\u901a <code>torch.Tensor</code>\uff08\u800c\u4e0d\u662f <code>RecorderTensor</code>\uff09\uff0c\u8fd9\u4e0d\u662f\u6211\u4eec\u671f\u671b\u7684\u884c\u4e3a\u3002</p> </li> </ul> <p>\u4e3a\u4e86\u907f\u514d\u8be5\u95ee\u9898\uff08\u5e76\u517c\u5bb9 PyTorch &lt; 1.9\uff09\uff0c\u6211\u4eec\u5728 <code>recorder_tensor.py</code> \u4e2d\u52a0\u5165\u4e86\u5982\u4e0b\u903b\u8f91\uff1a</p> <pre><code>        # This is necessary for torch version &lt; 1.10\n        if func in [F.linear, F.embedding]:\n            out = nn.parameter.Parameter.__torch_function__(\n                func, types, args, kwargs).as_subclass(RecorderTensor)\n        else:\n            # use original torch_function; otherwise,\n            # it leads to infinite recursive call of torch_function\n            out = super().__torch_function__(func, types, args, kwargs)\n</code></pre> <p>\u66f4\u7cbe\u786e\u5730\u8bf4\uff1a</p> <pre><code>F.linear \u5728 1.7.1\u30011.8\u30011.9 \u4f1a\u8fd4\u56de `NotImplemented`\nF.embedding \u5728 1.7.1\u30011.8\u30011.9 \u4f1a\u8fd4\u56de `NotImplemented`\n</code></pre> <p>\u6b64\u5916\uff0c\u672c\u5305\u4e0d\u652f\u6301 torch \u7248\u672c (\\le 1.6)\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u7248\u672c\u7684 <code>torch.Tensor</code> \u8fd8\u4e0d\u652f\u6301\u4ee5\u7c7b\u65b9\u6cd5\u5f62\u5f0f\u63d0\u4f9b <code>__torch_function__</code>\u3002</p> <ul> <li>\u5176\u5b83\u76f8\u5173 PR / issue\uff1a</li> <li>PR1</li> <li>PR2</li> </ul>"},{"location":"zh/developer/torch_function_notes/#meta-tensor","title":"Meta tensor \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>https://github.com/pytorch/pytorch/blob/orig/release/1.9/torch/_tensor.py</li> <li>https://github.com/pytorch/pytorch/issues/87990</li> </ul>"},{"location":"zh/tutorial/","title":"\u6982\u89c8","text":""},{"location":"zh/tutorial/#_1","title":"\u6559\u7a0b","text":"<p>\u672c\u90e8\u5206\u7528\u4e8e\u63d0\u4f9b torchview \u7684\u5165\u95e8\u6559\u7a0b\u4e0e\u793a\u4f8b\uff0c\u5e2e\u52a9\u4f60\u7406\u89e3\uff1a\u5982\u4f55\u6784\u9020\u8f93\u5165\u3001\u5982\u4f55\u8bfb\u53d6\u56fe\u4e2d\u7684\u8282\u70b9\u542b\u4e49\uff0c\u4ee5\u53ca\u5e38\u89c1\u53c2\u6570\uff08\u5982 <code>roll</code>\u3001<code>hide_inner_tensors</code>\uff09\u4f1a\u600e\u6837\u5f71\u54cd\u6700\u7ec8\u56fe\u3002</p>"},{"location":"zh/tutorial/example_introduction/","title":"\u793a\u4f8b\uff1a\u5165\u95e8\u4ecb\u7ecd","text":""},{"location":"zh/tutorial/example_introduction/#_1","title":"\u793a\u4f8b\uff1a\u5165\u95e8\u4ecb\u7ecd","text":"<p>\u672c\u6559\u7a0b\u7528\u4e8e\u4ecb\u7ecd <code>torchview</code> \u7684 API\u3001\u56fe\u4e2d\u8282\u70b9\u7684\u542b\u4e49\uff0c\u4ee5\u53ca\u51e0\u4e2a\u6700\u5e38\u7528\u7684\u53c2\u6570\u7ec4\u5408\u3002</p>"},{"location":"zh/tutorial/example_introduction/#0","title":"0. \u5b89\u88c5","text":"<p>\u4f60\u9700\u8981 Graphviz\uff08\u7528\u4e8e\u6e32\u67d3\u6700\u7ec8\u56fe\uff09\uff0c\u4ee5\u53ca torchview\uff1a</p> <pre><code>pip install graphviz\npip install torchview\n</code></pre> <p>\u5982\u679c\u7cfb\u7edf\u91cc\u8fd8\u6ca1\u6709 <code>dot</code> \u547d\u4ee4\uff0c\u8bf7\u6309\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5 Graphviz\uff08\u4f8b\u5982 Windows \u53ef\u7528 <code>choco install graphviz</code>\uff09\u3002</p>"},{"location":"zh/tutorial/example_introduction/#1-mlp","title":"1. \u7b2c\u4e00\u4e2a\u4f8b\u5b50\uff1aMLP","text":"<p>\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff1a</p> <pre><code>import torch\nfrom torch import nn\nfrom torchview import draw_graph\n\n\nclass MLP(nn.Module):\n    def __init__(self, inplace: bool = True) -&gt; None:\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(inplace),\n            nn.Linear(128, 128),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.layers(x)\n</code></pre> <p>\u7528 <code>draw_graph</code> \u751f\u6210\u53ef\u89c6\u5316\u56fe\uff1a</p> <pre><code>model_graph = draw_graph(\n    MLP(),\n    input_size=(2, 128),\n    graph_name=\"MLP\",\n    hide_inner_tensors=False,\n    hide_module_functions=False,\n)\nmodel_graph.visual_graph\n</code></pre> <p>\u8fd9\u91cc\u6211\u4eec\u628a <code>hide_inner_tensors=False</code>\u3001<code>hide_module_functions=False</code> \u6253\u5f00\uff0c\u662f\u4e3a\u4e86\u8ba9\u56fe\u5c55\u793a\u66f4\u591a\u7ec6\u8282\uff08\u66f4\u9002\u5408\u5b66\u4e60\uff09\u3002</p>"},{"location":"zh/tutorial/example_introduction/#2","title":"2. \u56fe\u91cc\u6709\u54ea\u4e9b\u8282\u70b9\uff1f","text":"<p>torchview \u7684\u56fe\u91cc\u4e3b\u8981\u6709\u4e09\u7c7b\u8282\u70b9\uff1a</p> <ul> <li>Tensor Node\uff1a\u8868\u793a\u5f20\u91cf\uff08\u8f93\u5165/\u4e2d\u95f4/\u8f93\u51fa\uff09</li> <li>Function Node\uff1a\u8868\u793a\u7b97\u5b50/\u51fd\u6570\u8c03\u7528\uff08\u4f8b\u5982 <code>torch.relu</code>\u3001<code>torch.add</code>\uff09</li> <li>Module Node\uff1a\u8868\u793a\u6a21\u5757\u8c03\u7528\uff08\u4f8b\u5982 <code>nn.Linear</code>\u3001<code>nn.Sequential</code>\uff09</li> </ul> <p>\u8282\u70b9\u6807\u7b7e\u901a\u5e38\u5305\u542b\uff1a\u8282\u70b9\u540d\u3001\u5c42\u7ea7\u6df1\u5ea6\u3001\u8f93\u5165/\u8f93\u51fa shape \u7b49\u4fe1\u606f\u3002</p>"},{"location":"zh/tutorial/example_introduction/#3-rolling","title":"3. Rolling\uff1a\u6298\u53e0\u9012\u5f52\u6a21\u5757","text":"<p>\u5f53\u6a21\u578b\u4e2d\u5b58\u5728\u201c\u91cd\u590d\u8c03\u7528\u540c\u4e00\u4e2a\u6a21\u5757\u5bf9\u8c61\u201d\uff08\u4f8b\u5982 RNN \u7684 cell \u53cd\u590d\u590d\u7528\uff09\u65f6\uff0c\u56fe\u53ef\u80fd\u4f1a\u975e\u5e38\u957f\u3002</p> <p>\u8fd9\u65f6\u53ef\u4ee5\u4f7f\u7528 <code>roll=True</code> \u5c06\u9012\u5f52\u7ed3\u6784\u6298\u53e0\uff1a</p> <pre><code>import torch\nfrom torch import nn\n\n\nclass SimpleRNN(nn.Module):\n    \"\"\"\u4e00\u4e2a\u7528\u4e8e\u6f14\u793a rolling \u7684\u7b80\u5316 RNN\uff08\u4f7f\u7528 LSTMCell \u53cd\u590d\u8fed\u4ee3\uff09\"\"\"\n\n    def __init__(self, inplace: bool = True) -&gt; None:\n        super().__init__()\n        self.hid_dim = 2\n        self.input_dim = 3\n        self.max_length = 4\n        self.lstm = nn.LSTMCell(self.input_dim, self.hid_dim)\n        self.activation = nn.LeakyReLU(inplace=inplace)\n\n    def forward(self, token_embedding: torch.Tensor) -&gt; torch.Tensor:\n        b_size = token_embedding.size()[0]\n        hx = torch.randn(b_size, self.hid_dim, device=token_embedding.device)\n        cx = torch.randn(b_size, self.hid_dim, device=token_embedding.device)\n\n        for _ in range(self.max_length):\n            hx, cx = self.lstm(token_embedding, (hx, cx))\n            hx = self.activation(hx)\n\n        return hx\n\nmodel_graph = draw_graph(\n    SimpleRNN(),\n    input_size=(2, 3),\n    graph_name=\"RecursiveNet\",\n    roll=True,\n)\nmodel_graph.visual_graph\n</code></pre> <p>\u6298\u53e0\u540e\uff0c\u56fe\u4e2d\u8fb9\u65c1\u8fb9\u7684\u6570\u5b57\u8868\u793a\u8be5\u8fb9\u5728 forward \u4e2d\u88ab\u201c\u91cd\u590d\u4f7f\u7528\u201d\u7684\u6b21\u6570\uff1b\u82e5\u6b21\u6570\u4e3a 1 \u5219\u901a\u5e38\u4e0d\u4f1a\u663e\u793a\u3002</p>"},{"location":"zh/tutorial/example_introduction/#4-resize","title":"4. Resize\uff1a\u7f29\u653e\u8f93\u51fa\u56fe","text":"<p>\u5982\u679c\u6e32\u67d3\u51fa\u6765\u7684\u56fe\u8fc7\u5927\uff0c\u53ef\u4ee5\u7f29\u653e\uff1a</p> <pre><code>model_graph.resize_graph(scale=0.5)\nmodel_graph.visual_graph\n</code></pre>"},{"location":"zh/tutorial/example_introduction/#5-vscode","title":"5. \u4e00\u4e2a\u5e38\u89c1\u7684\u5c0f\u5751\uff08VSCode \u6e32\u67d3\u88c1\u5207\uff09","text":"<p>\u5728 VSCode \u7684 Notebook \u6e32\u67d3\u91cc\uff0c\u8f83\u5927\u7684 SVG \u56fe\u53ef\u80fd\u88ab\u88c1\u5207\u3002\u53ef\u4ee5\u6539\u7528 PNG\uff1a</p> <pre><code>import graphviz\ngraphviz.set_jupyter_format(\"png\")\n</code></pre>"}]}